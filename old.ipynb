{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Image Data Loading Pipeline (v1)\n",
    "\n",
    "This module provides a custom PyTorch Dataset and DataLoader setup\n",
    "designed specifically for inference tasks where we need to process\n",
    "images and know their original file paths.\n",
    "\n",
    "It encapsulates all \"Pass 1\" logic:\n",
    "1. Recursive file scanning.\n",
    "2. Filtering by image extension.\n",
    "3. SHA256 hashing for de-duplication.\n",
    "4. Safe image loading (handles corrupt files, RGBA, etc.).\n",
    "\n",
    "It provides:\n",
    "- `InferenceDataset`: A PyTorch Dataset that finds all unique\n",
    "  images and returns (PIL_Image, file_path) tuples.\n",
    "- `collate_fn`: A custom collate function for the DataLoader that\n",
    "  batches images and paths separately, and isolates failures.\n",
    "\n",
    "USAGE (in another script):\n",
    "\n",
    "from image_dataloader import check_dependencies, InferenceDataset, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "try:\n",
    "    check_dependencies()\n",
    "    \n",
    "    SOURCE_PATH = r\"C:\\path\\to\\your\\photos\"\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    dataset = InferenceDataset(SOURCE_PATH)\n",
    "    print(\"Building file list (scanning and hashing)...\")\n",
    "    dataset.build_file_list()\n",
    "    print(f\"Found {len(dataset)} unique images.\")\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,  # Adjust based on your CPU\n",
    "        shuffle=False # Order doesn't matter for inference\n",
    "    )\n",
    "\n",
    "    for batch_num, (batch_images, batch_paths, failed_paths) in enumerate(loader):\n",
    "        print(f\"\\n--- Processing Batch {batch_num + 1} ---\")\n",
    "        print(f\"  Images to process: {len(batch_images)}\")\n",
    "        print(f\"  Paths: {batch_paths}\")\n",
    "        print(f\"  Failed to load: {failed_paths}\")\n",
    "        \n",
    "        # --- YOUR INFERENCE LOGIC HERE ---\n",
    "        # e.g., results_pass_1 = model_1(batch_images)\n",
    "        # ...then map results back to batch_paths to move files.\n",
    "\n",
    "except ImportError as e:\n",
    "    print(e, file=sys.stderr)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import hashlib\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# --- Dependency Imports (grouped for checking) ---\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    Image = None\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "except ImportError:\n",
    "    torch = None\n",
    "    Dataset = object # Dummy class for inheritance if torch is missing\n",
    "\n",
    "try:\n",
    "    import pillow_heif\n",
    "except ImportError:\n",
    "    pillow_heif = None\n",
    "\n",
    "# --- Constants ---\n",
    "IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.heic', '.heif')\n",
    "HASH_CHUNK_SIZE = 8192  # Read files in 8KB chunks for hashing\n",
    "\n",
    "# --- Prerequisite Functions ---\n",
    "\n",
    "def check_dependencies():\n",
    "    \"\"\"\n",
    "    Checks if all required libraries are installed.\n",
    "    Raises ImportError if a critical one is missing.\n",
    "    \"\"\"\n",
    "    print(\"Checking dependencies...\")\n",
    "    if Image is None:\n",
    "        raise ImportError(\"Error: 'Pillow' library not found. Please install with: pip install Pillow\")\n",
    "    if torch is None:\n",
    "        raise ImportError(\"Error: 'torch' library not found. Please install with: pip install torch\")\n",
    "    \n",
    "    if pillow_heif is None:\n",
    "        print(\"Warning: 'pillow-heif' library not found. HEIC/HEIF files will not be processed.\", file=sys.stderr)\n",
    "        print(\"Install with: pip install pillow-heif\", file=sys.stderr)\n",
    "    else:\n",
    "        # Register the HEIF plugin with PIL\n",
    "        pillow_heif.register_heif_opener()\n",
    "        print(\"HEIF/HEIC support enabled.\")\n",
    "    print(\"All critical dependencies are present.\")\n",
    "\n",
    "# --- Helper Functions (used by the Dataset) ---\n",
    "\n",
    "def _calculate_hash(file_path):\n",
    "    \"\"\"Calculates the SHA256 hash of a file efficiently.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            while chunk := f.read(HASH_CHUNK_SIZE):\n",
    "                hasher.update(chunk)\n",
    "        return hasher.hexdigest()\n",
    "    except (IOError, OSError) as e:\n",
    "        print(f\"   [Warning] Could not hash file {file_path}: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "@contextmanager\n",
    "def _suppress_pil_warnings():\n",
    "    \"\"\"Context manager to suppress known PIL warnings.\"\"\"\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        \"(Possibly corrupt EXIF data|Image size.*exceeds pixel limit)\"\n",
    "    )\n",
    "    yield\n",
    "    warnings.resetwarnings()\n",
    "\n",
    "def _load_image_for_batch(path):\n",
    "    \"\"\"\n",
    "    Safely opens, converts, and loads one image.\n",
    "    Returns None if the image fails to load.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _suppress_pil_warnings(), Image.open(path) as image:\n",
    "            if image.mode == 'RGBA':\n",
    "                # Convert RGBA to RGB (common for PNGs)\n",
    "                image_rgb = image.convert('RGB')\n",
    "            else:\n",
    "                # Load image data into memory to close the file handle\n",
    "                image.load()\n",
    "                image_rgb = image\n",
    "            return image_rgb\n",
    "    except Exception as e:\n",
    "        # We don't print here; we let the collate_fn report it\n",
    "        # print(f\"   [Error] Failed to load {path}: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "# --- Core Dataloader Components ---\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that scans a directory, de-duplicates files\n",
    "    based on their hash, and returns (PIL_Image, file_path) tuples\n",
    "    for use in an inference pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_path, image_extensions=IMAGE_EXTENSIONS):\n",
    "        if not os.path.isdir(root_path):\n",
    "            raise NotADirectoryError(f\"Root path not found: {root_path}\")\n",
    "            \n",
    "        self.root_path = root_path\n",
    "        self.image_extensions = image_extensions\n",
    "        self.unique_image_paths = []\n",
    "        self.seen_hashes = {}\n",
    "        \n",
    "        # Statistics\n",
    "        self.total_files_scanned = 0\n",
    "        self.skipped_duplicates = 0\n",
    "        self.hashing_errors = 0\n",
    "\n",
    "    def build_file_list(self):\n",
    "        \"\"\"\n",
    "        Performs the \"Pass 1\" logic to scan, hash, and de-duplicate\n",
    "        all images in the root_path. This must be called before\n",
    "        using the Dataset with a DataLoader.\n",
    "        \"\"\"\n",
    "        print(f\"Scanning {self.root_path} for images...\")\n",
    "        for dirpath, _, filenames in os.walk(self.root_path):\n",
    "            print(f\"  Scanning: {dirpath}\")\n",
    "            for filename in filenames:\n",
    "                if not filename.lower().endswith(self.image_extensions):\n",
    "                    continue\n",
    "                \n",
    "                self.total_files_scanned += 1\n",
    "                full_path = os.path.join(dirpath, filename)\n",
    "\n",
    "                file_hash = _calculate_hash(full_path)\n",
    "                if not file_hash:\n",
    "                    self.hashing_errors += 1\n",
    "                    continue\n",
    "\n",
    "                if file_hash in self.seen_hashes:\n",
    "                    # print(f\"   -> Duplicate of: {self.seen_hashes[file_hash]} (Skipping)\")\n",
    "                    self.skipped_duplicates += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    self.seen_hashes[file_hash] = full_path\n",
    "                    self.unique_image_paths.append(full_path)\n",
    "        \n",
    "        print(\"\\n--- Scan Complete ---\")\n",
    "        print(f\"Total files scanned: {self.total_files_scanned}\")\n",
    "        print(f\"Duplicate files found: {self.skipped_duplicates}\")\n",
    "        print(f\"Hashing/Read errors: {self.hashing_errors}\")\n",
    "        print(f\"Unique images to process: {len(self.unique_image_paths)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of unique images found.\"\"\"\n",
    "        return len(self.unique_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Loads one image and returns it with its original path.\n",
    "        Returns (None, path) if loading fails.\n",
    "        \"\"\"\n",
    "        file_path = self.unique_image_paths[index]\n",
    "        image = _load_image_for_batch(file_path)\n",
    "        return image, file_path\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for the InferenceDataLoader.\n",
    "    \n",
    "    Input:\n",
    "      - batch: A list of (image, file_path) tuples from __getitem__.\n",
    "               Some images may be None if loading failed.\n",
    "               \n",
    "    Output:\n",
    "      - A tuple of three lists:\n",
    "        1. batch_images: A list of successfully loaded PIL.Image objects.\n",
    "        2. batch_paths: A list of file paths corresponding to batch_images.\n",
    "        3. failed_paths: A list of file paths that failed to load.\n",
    "    \"\"\"\n",
    "    batch_images = []\n",
    "    batch_paths = []\n",
    "    failed_paths = []\n",
    "\n",
    "    for image, path in batch:\n",
    "        if image is not None:\n",
    "            batch_images.append(image)\n",
    "            batch_paths.append(path)\n",
    "        else:\n",
    "            failed_paths.append(path)\n",
    "            \n",
    "    return batch_images, batch_paths, failed_paths\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc312fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    This block demonstrates how to use the InferenceDataset\n",
    "    and collate_fn with a PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- CONFIGURE YOUR TEST PATH HERE ---\n",
    "    # IMPORTANT: Update this to a small test directory of images\n",
    "    SOURCE_PATH = r\"C:\\path\\to\\your\\test_photos\"\n",
    "    BATCH_SIZE = 8\n",
    "    \n",
    "    if \"C:\\\\path\\\\to\" in SOURCE_PATH:\n",
    "        print(\"Error: Please update the 'SOURCE_PATH' variable in the\", file=sys.stderr)\n",
    "        print(\"if __name__ == '__main__': block to point to a test folder.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        # 1. Check dependencies\n",
    "        check_dependencies()\n",
    "        \n",
    "        # 2. Create dataset and build the file list\n",
    "        dataset = InferenceDataset(SOURCE_PATH)\n",
    "        dataset.build_file_list()\n",
    "\n",
    "        if len(dataset) == 0:\n",
    "            print(\"\\nNo images found in the source path. Exiting.\")\n",
    "            sys.exit(0)\n",
    "\n",
    "        # 3. Create the DataLoader\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0,  # 0 is safer for testing. Increase to 2 or 4 for speed.\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Starting DataLoader Test (1 Batch) ---\")\n",
    "        \n",
    "        # 4. Loop through the loader\n",
    "        for i, (batch_images, batch_paths, failed_paths) in enumerate(loader):\n",
    "            print(f\"\\nSuccessfully loaded batch {i+1}\")\n",
    "            print(f\"  Images in batch: {len(batch_images)}\")\n",
    "            \n",
    "            if batch_images:\n",
    "                print(f\"  First image size: {batch_images[0].size}\")\n",
    "                print(f\"  First image path: {batch_paths[0]}\")\n",
    "\n",
    "            if failed_paths:\n",
    "                print(f\"  Failed to load {len(failed_paths)} images:\")\n",
    "                for path in failed_paths:\n",
    "                    print(f\"    - {path}\")\n",
    "            \n",
    "            # Stop after one batch for this demo\n",
    "            print(\"\\nDemo complete. This is where your inference logic would run.\")\n",
    "            break\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"ImportError: {e}\", file=sys.stderr)\n",
    "    except NotADirectoryError as e:\n",
    "        print(f\"Error: {e}\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\", file=sys.stderr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
